---
layout: single
title: "실시간 통신 (3) - 품질 관리와 적응 - soo:bak"
date: "2026-01-20 22:31:18 +0900"
description: Jitter Buffer, 패킷 손실 대응, FEC, 적응형 비트레이트, 에코 캔슬레이션, SFU/MCU를 설명합니다.
tags:
  - 네트워크
  - WebRTC
  - QoS
  - FEC
  - ABR
  - SFU
  - MCU
---

## 네트워크 변화에 어떻게 적응하는가

[Part 2](/dev/network/RealTimeCommunication-2/)에서 WebRTC가 시그널링, ICE, DTLS-SRTP를 통해 P2P 연결을 수립하는 과정을 살펴보았습니다.

<br>

연결이 수립되면 미디어가 흐르기 시작합니다. 그러나 네트워크는 고정된 환경이 아닙니다. 사용자가 Wi-Fi에서 LTE로 전환하고, 다른 트래픽과 대역폭을 공유하며, 라우터의 큐가 차고 비워지기를 반복합니다. 이러한 변화는 패킷 도착 시간의 변동(Jitter), 패킷 손실, 가용 대역폭 변동으로 나타납니다. 실시간 통신에서는 이러한 네트워크 변화에 즉각 대응해야 합니다. 패킷 손실, Jitter, 대역폭 변동에 적응하지 못하면 통화 품질은 급격히 저하됩니다. 이 문제를 해결하는 기법들을 살펴봅니다.

---

## Jitter Buffer

패킷 도착 시간의 **변동(Jitter)**을 흡수하여 안정적인 재생을 가능하게 합니다.

<br>

네트워크 지연은 일정하지 않습니다. 패킷이 도착하는 시간 간격이 들쭉날쭉하면 재생 시점에 필요한 패킷이 없어 화면과 소리가 끊기게 됩니다. Jitter Buffer는 패킷을 즉시 재생하지 않고 잠시 보관했다가 일정한 간격으로 꺼내 사용함으로써 이 문제를 해결합니다.

```
네트워크에서:
패킷 1 도착: 0ms
패킷 2 도착: 25ms (+5ms 지터)
패킷 3 도착: 35ms (-5ms 지터)
패킷 4 도착: 65ms (+5ms 지터)

직접 재생하면:
프레임 간격이 불규칙 → 끊김

Jitter Buffer 사용:
버퍼에서 일정 간격으로 꺼냄 → 부드러운 재생
```

위 예시는 Jitter Buffer가 도착 시간의 변동을 흡수하여 재생 간격을 균일하게 만드는 과정을 보여줍니다.

<br>

### 버퍼 크기 트레이드오프

버퍼 크기는 지연과 안정성 사이의 트레이드오프를 결정합니다.

```
작은 버퍼:
- 낮은 지연
- Jitter 흡수 능력 낮음
- 끊김 가능성 높음

큰 버퍼:
- 높은 지연
- Jitter 흡수 능력 높음
- 대화 지연 증가
```

고정된 버퍼 크기는 모든 네트워크 상황에 대응할 수 없습니다. 안정적인 네트워크에서 큰 버퍼를 사용하면 불필요한 지연이 발생하고, 불안정한 네트워크에서 작은 버퍼를 사용하면 재생이 자주 끊깁니다.

<br>

### 적응형 Jitter Buffer

네트워크 상태에 따라 버퍼 크기를 **동적으로** 조절하여 지연과 안정성의 균형을 맞춥니다.

<br>

적응형 Jitter Buffer는 네트워크의 Jitter를 실시간으로 측정하여 버퍼 크기를 동적으로 조절합니다.

```
Jitter 낮을 때: 버퍼 줄임 (지연 감소)
Jitter 높을 때: 버퍼 늘림 (안정성 증가)

┌─────────────────────────────────────────┐
│         적응형 버퍼 크기                │
│                                         │
│  버퍼  │     ────────────────           │
│  크기  │    /                \          │
│       │   /                  \         │
│       │  /                    \        │
│       └─────────────────────────────   │
│            네트워크 Jitter              │
└─────────────────────────────────────────┘
```

이 방식으로 안정적인 네트워크에서는 낮은 지연을, 불안정한 네트워크에서는 끊김 없는 재생을 유지할 수 있습니다.

---

## 패킷 손실 대응

네트워크에서 패킷이 손실되면 미디어에 빈 구멍이 생깁니다. 실시간 통신에서는 재전송을 기다릴 시간이 부족하므로, 여러 복구 기법을 함께 사용합니다.

### FEC (Forward Error Correction)

**순방향 오류 수정**은 원본 데이터와 함께 복구용 데이터를 미리 전송하여, 재전송 없이 손실을 복구합니다.

<br>

재전송(retransmission)은 손실된 패킷을 다시 요청하여 받는 방식이지만, RTT만큼의 시간이 걸립니다. RTT가 200ms라면 재전송된 패킷이 도착할 때는 이미 재생 시점을 놓친 뒤이므로, 실시간 통신에서는 효과가 제한적입니다. FEC는 이 문제를 피하기 위해 미리 복구 정보를 함께 전송하는 방식입니다.

```
원본 패킷:  P1  P2  P3  P4
FEC 패킷:   F1 (P1⊕P2)  F2 (P3⊕P4)

P2 손실 시:
P2 = P1 ⊕ F1 로 복구
```

위 예시는 XOR 연산을 통해 손실된 패킷을 복구하는 과정입니다. P1과 P2를 XOR한 값을 FEC 패킷 F1으로 전송하면, P2가 손실되더라도 P1과 F1을 XOR하여 P2를 복구할 수 있습니다. FEC는 대역폭을 추가로 소비하지만, 재전송 왕복 시간을 기다리지 않아도 되므로 실시간 통신에 적합합니다. 복구 패킷이 원본의 20-50%에 달하므로, 네트워크 상태에 따라 FEC 비율을 동적으로 조절하는 것이 일반적입니다.

<br>

FEC의 주요 장점은 재전송 없이 패킷을 복구하므로 RTT의 영향을 받지 않는다는 점입니다. 실시간 통신에서 RTT가 200ms를 넘으면 재전송된 패킷이 재생 시점에 늦을 수 있는데, FEC는 이 문제를 피할 수 있습니다. 반면 대역폭 오버헤드가 20-50%에 달하며, 연속적인 패킷 손실에는 취약합니다. WebRTC는 **Opus 코덱의 InBand FEC**를 지원합니다. 다음은 SDP에서 InBand FEC를 활성화하는 설정입니다.

```
a=fmtp:111 useinbandfec=1
```

이 설정을 통해 오디오 스트림에 FEC를 적용할 수 있습니다.

<br>

### Packet Concealment (은닉)

손실된 패킷의 내용을 **이전 데이터를 기반으로 추정**하여 재생 품질을 유지합니다.

<br>

FEC로도 복구할 수 없는 패킷이 있을 때, 은닉(concealment) 기법은 빈 자리를 눈에 띄지 않게 메우는 역할을 합니다. 사람의 청각과 시각은 짧은 결함에 관대하므로, 완벽한 복구 대신 눈치채기 어려운 수준의 보정을 목표로 합니다. Packet Concealment는 패킷 손실이 발생했을 때 마지막 수단으로 사용되는 기법으로, FEC나 NACK으로도 복구가 불가능한 상황에서 품질 저하를 최소화합니다.

<br>

오디오에서는 이전 샘플 반복(직전 오디오 샘플을 짧게 반복하여 무음을 피함), 보간(interpolation, 이전과 이후 샘플을 선형 보간하여 자연스러운 전환 생성), 무음 삽입(짧은 손실에는 무음을 넣어 처리) 등의 방법을 사용합니다. 비디오에서는 이전 프레임 반복(가장 간단한 방식으로 직전 프레임을 유지)이나 모션 보상 예측(이전 프레임의 움직임 벡터를 활용하여 다음 프레임 추정)을 사용합니다.

<br>

### NACK과 재전송

**NACK(Negative Acknowledgment)**를 통해 수신자가 손실된 패킷을 명시적으로 알리고 재전송을 요청합니다. FEC로 복구할 수 없거나 FEC가 적용되지 않은 스트림에서는 재전송을 사용합니다. NACK은 수신자가 시퀀스 번호를 확인하여 누락된 패킷을 발견하면, 즉시 송신자에게 재전송을 요청하는 방식입니다.

```
수신자                         송신자
  │                              │
  │   패킷 1,2,4 수신 (3 손실)    │
  │                              │
  │ ── RTCP NACK (seq=3) ──────► │
  │                              │
  │ ◄──── 패킷 3 재전송 ──────── │
  │                              │
```

위 과정은 수신자가 패킷 3의 손실을 감지하고 재전송을 요청하여 받는 흐름을 보여줍니다.

<br>

RTT가 낮을 때만 효과적이며, RTT가 높으면 재전송된 패킷이 재생 시점을 놓칠 수 있습니다.

---

## 적응형 비트레이트 (ABR)

네트워크 상태에 따라 **인코딩 품질**을 동적으로 조절하여 끊김 없는 재생을 유지합니다. 패킷 손실 대응이 이미 발생한 문제를 복구하는 방식이라면, 적응형 비트레이트는 문제가 발생하기 전에 전송량 자체를 조절하여 예방하는 방식입니다.

<br>

### 대역폭 추정

RTCP의 피드백을 활용하여 가용 대역폭을 추정합니다.

<br>

적응형 비트레이트를 구현하려면 먼저 현재 네트워크에서 사용 가능한 대역폭을 알아야 합니다. 송신자는 RTCP Receiver Report를 통해 수신자의 패킷 손실률과 Jitter 정보를 받아, 네트워크 상태를 추정합니다.

```
송신자                         수신자
  │                              │
  │ ─── RTP 패킷들 ────────────► │
  │                              │
  │ ◄── RTCP Receiver Report ─── │
  │     (손실률, Jitter)         │
  │                              │
  │     손실률 높음 → 대역폭 낮춤 │
  │     손실률 낮음 → 대역폭 올림 │
```

위 흐름은 송신자가 피드백을 받아 비트레이트를 조절하는 과정을 나타냅니다.

<br>

WebRTC는 **Google Congestion Control (GCC)**을 사용합니다. GCC는 손실 기반(loss-based)과 지연 기반(delay-based) 추정을 결합하여 네트워크 혼잡을 감지하고, 이에 따라 전송 속도를 조절합니다.

<br>

### 인코더 조절

추정된 대역폭에 맞춰 인코더 파라미터를 조절합니다.

<br>

대역폭이 부족하면 비트레이트를 낮추고, 여유가 있으면 높입니다. 비트레이트 조절과 함께 해상도와 프레임레이트도 함께 조정하여 최적의 품질을 유지합니다.

```
대역폭 높음:
- 비트레이트: 2 Mbps
- 해상도: 1280x720
- 프레임레이트: 30fps

대역폭 낮음:
- 비트레이트: 500 Kbps
- 해상도: 640x360
- 프레임레이트: 15fps
```

위 예시는 네트워크 상태에 따라 인코더 파라미터를 조절하는 방식을 보여줍니다.

<br>

### Simulcast

송신자가 **여러 품질**의 스트림을 동시에 인코딩하여 전송합니다.

<br>

단일 비트레이트 조절 방식은 송신자가 하나의 품질만 전송하므로, 여러 수신자의 대역폭이 다를 때 모두를 만족시킬 수 없습니다. 예를 들어, 한 수신자는 고속 Wi-Fi로 연결되어 있고 다른 수신자는 느린 LTE로 연결되어 있다면, 송신자가 하나의 비트레이트만 선택할 경우 둘 중 하나는 불만족스러운 품질을 받게 됩니다. Simulcast는 송신자가 동일한 영상을 여러 해상도와 비트레이트로 인코딩하여 동시에 전송하고, 서버(SFU)가 각 수신자의 네트워크 상황에 맞는 스트림을 선택하여 전달합니다.

```
┌───────────────────────────────────────────────────────────┐
│                        송신자                             │
│                                                           │
│   ┌─────────┐   ┌─────────┐   ┌─────────┐                │
│   │ High    │   │ Medium  │   │ Low     │                │
│   │ 720p    │   │ 360p    │   │ 180p    │                │
│   └────┬────┘   └────┬────┘   └────┬────┘                │
│        │             │             │                     │
└────────┼─────────────┼─────────────┼─────────────────────┘
         │             │             │
         ▼             ▼             ▼
┌───────────────────────────────────────────────────────────┐
│                     미디어 서버 (SFU)                     │
│                                                           │
│   수신자 A: 대역폭 높음 → High 스트림 전달               │
│   수신자 B: 대역폭 낮음 → Low 스트림 전달                │
└───────────────────────────────────────────────────────────┘
```

위 구조는 송신자가 3개의 품질 레벨을 전송하고, SFU가 수신자별로 적절한 스트림을 선택하는 과정을 보여줍니다.

<br>

### SVC (Scalable Video Coding)

**계층적 인코딩**을 통해 하나의 스트림에서 여러 품질 수준을 추출할 수 있습니다.

<br>

Simulcast는 여러 스트림을 독립적으로 인코딩하므로 송신자의 CPU 부담이 크고, 전송 대역폭도 높습니다. 예를 들어, 720p, 360p, 180p 3개의 스트림을 동시에 인코딩하면 CPU 사용량이 단일 스트림 대비 3배 가까이 증가합니다. SVC(Scalable Video Coding)는 이 문제를 해결하기 위해 하나의 스트림을 계층 구조로 인코딩하여, 수신자가 필요한 계층만 선택적으로 받을 수 있도록 합니다.

```
SVC 스트림:
┌─────────────────────────────────────┐
│  Enhancement Layer 2 (High)        │
├─────────────────────────────────────┤
│  Enhancement Layer 1 (Medium)       │
├─────────────────────────────────────┤
│  Base Layer (Low)                   │
└─────────────────────────────────────┘

대역폭에 따라:
- 높음: 모든 레이어 수신
- 중간: Base + Enhancement 1
- 낮음: Base만 수신
```

Base Layer는 가장 낮은 품질이지만 모든 수신자가 재생할 수 있는 최소 정보를 담고 있으며, Enhancement Layer는 추가 품질 향상을 제공합니다.

---

## 에코 캔슬레이션

### 음향 에코의 원인

네트워크 품질 외에도 오디오 품질에 영향을 미치는 요소가 있습니다. 그중 하나가 에코(echo)입니다. 화상 회의나 VoIP 통화에서 스피커를 사용하면, 상대방의 목소리가 스피커를 통해 나온 뒤 다시 마이크로 들어가는 현상이 발생합니다. 이를 음향 에코(Acoustic Echo)라고 하며, 상대방은 자신의 목소리가 지연되어 되돌아오는 것을 듣게 됩니다. 이어폰을 사용하면 에코가 발생하지 않지만, 스피커폰이나 회의실 환경에서는 에코가 필연적으로 발생합니다.

```
┌─────────────────────────────────────────────────────────────┐
│                                                             │
│   스피커 ─────────────────► 마이크                         │
│      │                        ▲                            │
│      │      음향 반사         │                            │
│      └────────────────────────┘                            │
│                                                             │
│   상대방 목소리가 스피커 → 반사 → 마이크 → 상대방에게 다시 │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

위 구조는 에코가 발생하는 경로를 보여줍니다. 이러한 에코는 대화를 크게 방해합니다.

<br>

### AEC (Acoustic Echo Cancellation)

WebRTC는 소프트웨어 기반 AEC를 기본으로 제공합니다. AEC(Acoustic Echo Cancellation)는 스피커로 출력한 신호를 미리 알고 있으므로, 마이크 입력에서 해당 신호의 반향을 찾아내어 제거하는 방식으로 동작합니다.

```
원리:
1. 스피커로 보낸 신호를 기억
2. 마이크 입력에서 해당 신호의 반사를 추정
3. 추정된 에코를 빼서 제거

┌────────────┐
│ 스피커     │──────────────────────────┐
│ 출력       │                          │
└─────┬──────┘                          │
      │                                 │
      ▼                                 ▼
┌────────────┐    ┌────────────┐   ┌────────────┐
│ 에코 추정  │────│ 에코 빼기  │───│ 출력       │
│            │    │            │   │ (에코 제거)│
└────────────┘    └────────────┘   └────────────┘
                        ▲
                        │
                  ┌─────┴──────┐
                  │ 마이크     │
                  │ 입력       │
                  └────────────┘
```

위 흐름은 스피커 출력 신호를 참조하여 마이크 입력에서 에코를 제거하는 과정을 나타냅니다.

---

## 네트워크 문제 진단

### getStats() API

WebRTC 연결의 실시간 통계를 조회할 수 있습니다. WebRTC는 연결 품질을 모니터링할 수 있도록 getStats() API를 제공합니다. 이 API는 패킷 손실, Jitter, 프레임 드롭 등 다양한 지표를 실시간으로 반환합니다.

```javascript
const stats = await peerConnection.getStats();

stats.forEach(report => {
  if (report.type === 'inbound-rtp' && report.kind === 'video') {
    console.log('Packets received:', report.packetsReceived);
    console.log('Packets lost:', report.packetsLost);
    console.log('Jitter:', report.jitter);
    console.log('Frames decoded:', report.framesDecoded);
    console.log('Frames dropped:', report.framesDropped);
  }
});
```

위 코드는 비디오 수신 스트림의 통계를 조회하는 예시입니다. 반환된 지표를 분석하여 품질 문제를 진단할 수 있습니다.

<br>

### 품질 지표 해석

getStats()로 수집한 지표를 해석하면 현재 통화 품질을 판단할 수 있습니다.

<br>

**패킷 손실률**은 전송된 패킷 중 수신자에게 도달하지 못한 비율입니다. 1% 미만이면 우수한 상태이며, 1-5%는 수용 가능하지만 FEC나 은닉 기법이 활성화됩니다. 5% 이상이면 품질 저하가 심각하여 비디오 정지나 오디오 끊김이 발생합니다.

<br>

**Jitter**는 패킷 도착 시간의 변동을 나타냅니다. 30ms 미만이면 우수하고, 30-50ms는 수용 가능하지만 Jitter Buffer 크기가 증가합니다. 50ms 이상이면 버퍼가 불안정해져 문제가 됩니다.

<br>

**RTT**는 패킷이 왕복하는 데 걸리는 시간입니다. 150ms 미만이면 대화에 적합하며, 300ms 이상이면 대화가 어색해지고 말이 겹치는 현상이 자주 발생합니다.

<br>

### 일반적인 문제들

WebRTC 통화에서 자주 발생하는 문제들은 대부분 네트워크 또는 시스템 자원 상태로 원인을 추적할 수 있습니다. getStats()로 수집한 지표와 아래 패턴을 대조하면 문제를 빠르게 진단할 수 있습니다. 비디오가 정지하는 경우는 높은 패킷 손실, 키프레임 손실, 네트워크 대역폭 부족이 주요 원인입니다. 오디오가 끊기는 경우는 Jitter 증가, 버퍼 언더런(buffer underrun), CPU 과부하가 원인일 가능성이 높습니다. 단방향 오디오(한쪽만 들림) 문제는 NAT 트래버설 실패, 방화벽 차단, ICE 후보 교환 문제로 발생합니다.

---

## SFU vs MCU

다자간 화상 회의에서는 여러 참가자의 미디어를 어떻게 전달할 것인가가 중요한 아키텍처 결정입니다. 두 가지 주요 방식이 있습니다.

### MCU (Multipoint Control Unit)

**중앙 서버에서 모든 미디어를 디코딩하고 합성**하여 하나의 스트림으로 제공합니다.

<br>

각 참가자가 다른 모든 참가자의 스트림을 개별적으로 받으면, 클라이언트의 대역폭과 CPU 부담이 참가자 수에 비례하여 증가합니다. 10명이 참가한 회의라면 각 클라이언트가 9개의 스트림을 동시에 수신하고 디코딩해야 합니다. MCU는 이 문제를 해결하기 위해 서버에서 모든 스트림을 하나로 합성하여 전달합니다.

```
┌───────────────────────────────────────────────────────────┐
│                          MCU                              │
│                                                           │
│   Alice ─────►│                    │◄────── Bob          │
│               │   ┌────────────┐   │                      │
│               │───│  합성      │───│                      │
│               │   │ (디코딩+   │   │                      │
│   Carol ─────►│   │ 믹싱+     │   │◄────── Dave         │
│               │   │ 재인코딩) │   │                      │
│               │   └────────────┘   │                      │
│                                                           │
│   각 참가자는 합성된 하나의 스트림 수신                   │
└───────────────────────────────────────────────────────────┘
```

위 구조는 MCU가 모든 참가자의 스트림을 받아 합성한 뒤, 하나의 스트림으로 재전송하는 과정을 보여줍니다. MCU는 모든 미디어를 서버에서 디코딩한 뒤 하나의 화면으로 합성하여 재인코딩하므로, 클라이언트는 항상 하나의 스트림만 수신하면 됩니다. 대역폭이 제한된 클라이언트에게 유리하지만, 그만큼 서버에 집중되는 연산 부담이 큽니다.

<br>

MCU의 주요 장점은 클라이언트가 항상 하나의 스트림만 수신하므로, 참가자 수와 관계없이 대역폭이 일정하다는 점입니다. 또한 서버에서 레이아웃을 제어할 수 있어 클라이언트 구현이 단순해집니다. 반면 서버가 모든 미디어를 디코딩하고 믹싱하여 재인코딩해야 하므로 CPU 사용량이 높고, 처리 시간으로 인해 지연이 증가하며, 이로 인해 확장성이 제한됩니다.

<br>

### SFU (Selective Forwarding Unit)

미디어를 디코딩하거나 믹싱하지 않고 **선택적으로 전달**만 합니다.

<br>

MCU는 서버에서 디코딩과 인코딩을 수행하므로 CPU 부담이 큽니다. SFU(Selective Forwarding Unit)는 이 문제를 피하기 위해 미디어를 디코딩하지 않고, 패킷을 그대로 필요한 수신자에게 전달만 합니다.

```
┌───────────────────────────────────────────────────────────┐
│                          SFU                              │
│                                                           │
│   Alice ─────►│                    │──────► Bob          │
│               │   ┌────────────┐   │  (Alice+Carol 수신) │
│               │   │  라우팅    │   │                      │
│               │   │ (디코딩X)  │   │                      │
│   Carol ─────►│   └────────────┘   │──────► Alice        │
│               │                    │  (Bob+Carol 수신)   │
│                                                           │
│   각 참가자는 다른 참가자 스트림을 개별 수신              │
└───────────────────────────────────────────────────────────┘
```

위 구조는 SFU가 패킷을 단순 전달하는 방식으로 동작하는 과정을 보여줍니다. SFU는 미디어를 디코딩하지 않고 패킷 수준에서 라우팅만 하므로, 서버 비용이 MCU에 비해 크게 낮습니다. 현재 대부분의 화상 회의 서비스(Google Meet, Zoom 등)가 SFU 기반 아키텍처를 채택하고 있으며, Simulcast와 결합하여 각 수신자에게 적절한 품질의 스트림을 전달합니다.

<br>

SFU의 주요 장점은 미디어를 디코딩하지 않고 포워딩만 하므로 서버 부하가 낮고, 처리 지연이 최소화되며, 확장성이 좋다는 점입니다. 대규모 회의에서도 서버 비용이 합리적으로 유지됩니다. 반면 각 클라이언트가 N-1개의 스트림을 개별적으로 수신하고 디코딩해야 하므로, 클라이언트 대역폭과 CPU 사용량이 참가자 수에 비례하여 증가합니다.

<br>

### 확장성 트레이드오프

MCU와 SFU는 서로 다른 확장성 특성을 갖습니다. 다음 표는 참가자 수에 따른 부하를 비교합니다.

```
참가자 수    MCU 부하    SFU 부하    클라이언트 대역폭(SFU)
    2           낮음        낮음         1 스트림
    5           중간        낮음         4 스트림
   10           높음        낮음         9 스트림
   50          매우 높음    중간        49 스트림
```

MCU는 참가자 수가 늘어날수록 서버 부하가 급격히 증가하지만, SFU는 비교적 완만하게 증가합니다. 다만 SFU는 클라이언트가 다중 스트림을 처리해야 하므로, 클라이언트 대역폭이 참가자 수에 비례합니다. 대규모 회의는 **Simulcast + SFU** 조합이 일반적입니다.

---

## 마무리

실시간 통신에서 품질 관리는 고정된 설정이 아니라 지속적인 적응의 과정입니다. 네트워크는 끊임없이 변화하며, Jitter, 패킷 손실, 대역폭 변동은 예측할 수 없는 순간에 발생합니다. 이러한 환경에서 안정적인 통화를 유지하려면, 실시간으로 네트워크 상태를 감지하고 대응하는 메커니즘이 필수적입니다.

<br>

| 문제 | 해결책 |
|-----|-------|
| Jitter | 적응형 Jitter Buffer |
| 패킷 손실 | FEC, NACK, Concealment |
| 대역폭 변동 | 적응형 비트레이트 |
| 에코 | AEC (음향 에코 제거) |
| 확장성 | SFU + Simulcast |

<br>

WebRTC는 이러한 기법들을 통합하여 브라우저에서 별도의 플러그인 없이 실시간 통신을 제공합니다. Part 1에서 다룬 RTP 프로토콜이 미디어를 전송하는 기본 틀을 제공했다면, Part 2의 시그널링과 연결 수립 과정이 통신 경로를 만들었고, 이번 Part 3에서 살펴본 품질 관리 기법들이 그 경로 위에서 실제로 사용 가능한 수준의 통화 품질을 보장합니다. 네트워크가 완벽할 수 없다면, 불완전함에 적응하는 것이 최선의 전략입니다.

---

**관련 글**
- [네트워크 성능과 최적화 (1) - 지연 시간의 구성 요소](/dev/network/NetworkPerformance-1/)

**시리즈**
- [실시간 통신 (1) - RTP와 실시간 전송](/dev/network/RealTimeCommunication-1/)
- [실시간 통신 (2) - WebRTC 스택](/dev/network/RealTimeCommunication-2/)
- 실시간 통신 (3) - 품질 관리와 적응 (현재 글)
