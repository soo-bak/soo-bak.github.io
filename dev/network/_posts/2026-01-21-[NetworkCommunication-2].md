---
layout: single
title: "네트워크 통신의 원리 (2) - 디지털 신호와 정보 전달 - soo:bak"
date: "2026-01-21 09:00:00 +0900"
description: 아날로그-디지털 변환, 정보 이론의 핵심, 오류 검출과 정정, 다중 접근 방식을 설명합니다.
tags:
  - 네트워크
  - 디지털신호
  - 정보이론
  - 오류정정
---

## 왜 디지털인가

[Part 1](/dev/network/NetworkCommunication-1/)에서 물리적 신호 전송의 원리를 살펴보았습니다.

아날로그 신호를 그대로 전송할 수도 있는데, 왜 현대 통신은 디지털을 사용할까요?

<br>

아날로그 신호를 장거리 전송하면 어떤 일이 일어나는지 생각해봅시다.

신호는 거리에 따라 약해지고(감쇠), 잡음이 더해집니다.

신호를 증폭하면 어떻게 될까요?

원래 신호와 함께 **잡음도 증폭**됩니다.

증폭을 반복할수록 잡음의 비율이 커지고, 결국 원래 신호를 구분할 수 없게 됩니다.

```
원본 아날로그 신호:    ∿∿∿∿
          ↓ (잡음 추가)
약해진 신호:          ∿+잡음∿
          ↓ (증폭)
증폭된 신호:          ∿+잡음∿  (잡음도 함께 증폭)
          ↓ (또 잡음 추가)
          ↓ (또 증폭)
결과:                 잡음에 묻힌 신호
```

아날로그 테이프를 복사하면 음질이 떨어지고, 그 복사본을 다시 복사하면 더 떨어지는 것과 같은 원리입니다.

<br>

디지털 신호는 다릅니다.

0과 1, 두 가지 상태만 구분하면 됩니다.

신호가 약해지고 잡음이 더해져도, "이것이 0인가 1인가"만 판단할 수 있으면 됩니다.

잡음이 임계값을 넘지 않는 한, **원래 신호를 완벽하게 복원**할 수 있습니다.

```
전송된 디지털 신호:   ──┐    ┌──┐
                        │    │  │
                        └────┘  └──
                       0    1    0

수신된 신호 (잡음 포함):
                     ~~┐~~┌~~┐~~
                       │~  │ ~│
                       └~~ └~~└~~
                       ?    ?    ?

임계값으로 판단:
                     ──┐    ┌──┐
                        │    │  │
                        └────┘  └──
                       0    1    0  (원본 복원)
```

디지털 통신의 핵심은 이것입니다.

**연속적인 값을 전송하는 대신, 이산적인 심볼을 전송하고, 수신측에서 어떤 심볼인지 판단한다.**

---

## 아날로그를 디지털로 바꾸기

하지만 세상의 대부분의 정보는 아날로그입니다.

음성, 음악, 영상, 온도, 압력... 모두 연속적인 값입니다.

이것을 디지털로 변환해야 합니다.

<br>

### 표본화: 시간을 자르다

첫 번째 단계는 **표본화(Sampling)**입니다.

연속적으로 변하는 신호를 일정 간격으로 측정합니다.

```
아날로그 신호:    ∿∿∿∿∿∿∿∿∿∿
                  │ │ │ │ │ │ │ │
샘플링:           ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓
측정값:           3.2, 4.1, 4.8, 4.2, 3.1, 2.5, 2.8, 3.5
```

여기서 중요한 질문이 생깁니다.

**얼마나 자주 측정해야 원래 신호를 복원할 수 있을까요?**

<br>

1928년, 해리 나이퀴스트는 이 질문에 답했습니다.

신호에 포함된 최대 주파수의 **2배 이상**으로 샘플링하면 원래 신호를 완벽하게 복원할 수 있습니다.

```
샘플링 주파수 ≥ 2 × 신호의 최대 주파수
```

이것이 **나이퀴스트 정리(Nyquist Theorem)**입니다.

<br>

왜 2배일까요?

파동의 한 주기에는 최소한 2개의 점이 있어야 "오르내림"을 구분할 수 있습니다.

1개 점만 있으면 그것이 파동의 꼭대기인지 바닥인지 알 수 없습니다.

<br>

사람의 음성은 약 300Hz ~ 3400Hz 범위입니다.

전화 품질로 전송하려면 3400 × 2 = 6800Hz 이상으로 샘플링해야 합니다.

실제 전화 시스템은 8000Hz를 사용합니다.

<br>

CD 음질은 20Hz ~ 20kHz (사람 청각 범위)를 재현합니다.

20000 × 2 = 40000Hz 이상이 필요하고, 실제로는 44100Hz를 사용합니다.

<br>

**만약 나이퀴스트 조건을 만족하지 않으면 어떻게 될까요?**

고주파 신호가 저주파 신호로 잘못 해석됩니다.

이것을 **앨리어싱(Aliasing)**이라고 합니다.

영화에서 자동차 바퀴가 거꾸로 도는 것처럼 보이는 현상이 앨리어싱의 예입니다.

프레임 레이트(샘플링)가 바퀴 회전 주파수의 2배보다 낮기 때문입니다.

<br>

### 양자화: 크기를 자르다

표본화로 시간 축을 이산화했습니다.

이제 크기 축도 이산화해야 합니다.

**양자화(Quantization)**는 연속적인 크기 값을 유한한 개수의 레벨로 변환합니다.

```
연속값:    3.7  →  양자화  →  4
           2.3  →  양자화  →  2
           5.8  →  양자화  →  6
```

레벨의 개수는 사용하는 비트 수에 따라 결정됩니다.

8비트를 사용하면 2⁸ = 256개의 레벨을 표현할 수 있습니다.

16비트를 사용하면 2¹⁶ = 65536개의 레벨을 표현할 수 있습니다.

<br>

양자화는 필연적으로 정보 손실을 일으킵니다.

실제 값 3.7을 4로 표현하면 0.3의 오차가 발생합니다.

이것을 **양자화 오류(Quantization Error)** 또는 **양자화 잡음**이라고 합니다.

<br>

비트 수가 많을수록 레벨 간격이 촘촘해지고, 양자화 오류가 줄어듭니다.

CD 음질이 16비트를 사용하는 이유입니다.

8비트로는 256단계밖에 표현하지 못해 음질 저하가 느껴집니다.

<br>

### 부호화: 숫자를 비트로

마지막으로 양자화된 값을 이진수로 표현합니다.

이것을 **부호화(Encoding)**라고 합니다.

가장 기본적인 방식은 **PCM(Pulse Code Modulation)**입니다.

```
양자화 값 156
   ↓
이진수 변환
   ↓
10011100 (8비트)
```

<br>

전체 과정을 정리하면:

```
아날로그 신호 → 샘플링 → 양자화 → 부호화 → 디지털 비트열
   ∿∿∿         3.7      4       00000100    01010011...
```

<br>

이제 데이터율을 계산할 수 있습니다.

```
데이터율 = 샘플링 주파수 × 비트 깊이 × 채널 수
```

CD 오디오:
```
44100 Hz × 16비트 × 2채널 = 1,411,200 bps ≈ 1.4 Mbps
```

1분 음악은 약 10MB가 됩니다.

---

## 비트를 물리 신호로: 라인 코딩

디지털 데이터(0과 1의 나열)를 물리적 매체로 전송하려면, 비트를 전압이나 전류 변화로 바꿔야 합니다.

이것을 **라인 코딩(Line Coding)**이라고 합니다.

<br>

가장 단순한 방법은 0을 0V, 1을 5V로 표현하는 것입니다.

이것을 **NRZ(Non-Return to Zero)**라고 합니다.

```
비트:    1   0   0   1   1   1   0   0   0   0
        ┌─┐           ┌───────┐
전압:   │ │           │       │
       ─┘ └───────────┘       └───────────────
```

<br>

간단해 보이지만 문제가 있습니다.

**동기화** 문제입니다.

수신측은 언제 한 비트가 끝나고 다음 비트가 시작되는지 알아야 합니다.

위 예시에서 마지막 0이 4개인지 5개인지 어떻게 알 수 있을까요?

송신측과 수신측의 클럭이 조금만 달라도 비트 경계가 어긋납니다.

<br>

NRZ의 또 다른 문제는 **DC 성분**입니다.

1이 계속되면 전압이 계속 높은 상태를 유지합니다.

이런 직류 성분은 일부 전송 매체에서 문제를 일으킵니다.

<br>

**맨체스터 인코딩(Manchester Encoding)**은 이 문제들을 해결합니다.

각 비트 중간에서 반드시 전압이 바뀝니다.

```
비트:       1       0       1       1       0
맨체스터:  ┌─┐     ┌─┐     ┌─┐     ┌─┐     ┌─┐
          │ └─┐ ┌─┘ │ │ └─┐ │ └─┐ ┌─┘ │
          └───┘ └───┘ └───┘ └───┘ └───┘

1: 비트 중간에서 고→저 전이
0: 비트 중간에서 저→고 전이
```

비트 중간에 항상 전이가 있으므로 수신측은 이를 이용해 동기화할 수 있습니다.

또한 고전압과 저전압이 번갈아 나타나므로 DC 성분이 없습니다.

<br>

대신 맨체스터 인코딩은 더 많은 대역폭을 사용합니다.

한 비트를 표현하는 데 두 번의 전압 변화가 필요하기 때문입니다.

10 Mbps 이더넷(10BASE-T)은 맨체스터 인코딩을 사용하지만, 100 Mbps 이상에서는 더 효율적인 방식을 사용합니다.

<br>

**4B/5B 인코딩**은 효율과 동기화의 절충안입니다.

4비트 데이터를 5비트 코드로 변환합니다.

32개의 5비트 코드 중에서 전이가 자주 일어나는 16개만 선택하여 사용합니다.

80% 효율(4/5)로 동기화를 보장합니다.

100 Mbps 이더넷(100BASE-TX)이 이 방식을 사용합니다.

---

## 정보란 무엇인가

1948년, 벨 연구소의 클로드 섀넌은 놀라운 질문을 던졌습니다.

**"정보"를 수학적으로 정의할 수 있을까?**

<br>

섀넌의 통찰은 이것입니다.

**정보는 불확실성의 해소다.**

<br>

동전을 던져 앞면이 나왔다는 메시지를 생각해봅시다.

이 메시지는 "앞면 또는 뒷면" 중 하나를 알려줍니다.

두 가지 가능성 중 하나를 확정했으므로, 1비트의 정보를 전달한 것입니다.

<br>

주사위를 던져 3이 나왔다는 메시지는 어떨까요?

여섯 가지 가능성 중 하나를 확정했습니다.

log₂(6) ≈ 2.58비트의 정보를 전달한 것입니다.

<br>

이것을 일반화하면:

```
정보량 = -log₂(확률)
```

확률이 낮은(예상하기 어려운) 사건일수록 더 많은 정보를 담고 있습니다.

"내일 해가 뜬다"는 거의 확실하므로 정보량이 낮습니다.

"내일 눈이 온다"는 (여름이라면) 놀라운 사건이므로 정보량이 높습니다.

<br>

**엔트로피(Entropy)**는 정보원의 평균 정보량입니다.

```
H = -Σ P(x) × log₂(P(x))
```

영어 텍스트를 생각해봅시다.

알파벳 26개가 동일한 확률로 나온다면, 글자당 log₂(26) ≈ 4.7비트가 필요합니다.

하지만 실제 영어에서 'e'는 자주 나오고 'z'는 드물게 나옵니다.

이런 불균등한 분포를 고려하면, 영어 텍스트의 엔트로피는 글자당 약 1 ~ 1.5비트입니다.

<br>

이것이 의미하는 바는 강력합니다.

**영어 텍스트는 글자당 1.5비트 정도의 정보만 담고 있다.**

나머지는 중복(Redundancy)입니다.

그래서 압축이 가능합니다.

<br>

### 섀넌의 채널 용량

섀넌의 가장 중요한 발견은 **채널 용량(Channel Capacity)**입니다.

잡음이 있는 채널에서 오류 없이 전송할 수 있는 최대 속도가 존재한다는 것입니다.

```
C = B × log₂(1 + S/N)
```

[Part 1](/dev/network/NetworkCommunication-1/)에서 소개한 이 공식의 심오한 의미를 더 살펴봅시다.

<br>

이 정리에는 두 가지 놀라운 주장이 담겨 있습니다.

**첫째**, C보다 낮은 어떤 전송 속도도 달성할 수 있습니다.

적절한 코딩을 사용하면 오류를 원하는 만큼 낮출 수 있습니다.

**둘째**, C를 초과하는 전송 속도는 불가능합니다.

어떤 방법을 써도 오류가 필연적으로 발생합니다.

<br>

섀넌은 한계가 존재한다는 것을 증명했지만, 그 한계에 도달하는 구체적인 방법은 제시하지 않았습니다.

이후 수십 년간 공학자들은 이 한계에 접근하는 코딩 방법을 개발했습니다.

1993년에 발명된 터보 코드(Turbo Code)는 섀넌 한계의 99% 이상에 도달했습니다.

---

## 오류는 피할 수 없다

물리적 채널에는 항상 잡음이 있습니다.

열 잡음(전자의 무작위 운동), 간섭(다른 신호), 감쇠(거리에 따른 신호 약화) 등이 원인입니다.

잡음으로 인해 0이 1로, 1이 0으로 잘못 수신될 수 있습니다.

<br>

오류를 완전히 제거할 수는 없습니다.

하지만 **검출**하거나 **정정**할 수 있습니다.

핵심 아이디어는 **중복(Redundancy)**입니다.

<br>

### 패리티 비트: 가장 간단한 오류 검출

가장 간단한 방법은 **패리티 비트(Parity Bit)**입니다.

데이터에 1비트를 추가하여 전체 1의 개수가 짝수(또는 홀수)가 되게 합니다.

```
데이터: 1011001 (1이 4개, 짝수)
패리티 비트: 0
전송: 10110010

데이터: 1011101 (1이 5개, 홀수)
패리티 비트: 1
전송: 10111011
```

수신측에서 1의 개수를 세어 짝수가 아니면 오류가 발생한 것을 알 수 있습니다.

<br>

패리티의 한계는 명확합니다.

1비트 오류는 검출할 수 있지만, 2비트 오류는 검출하지 못합니다.

(1이 2개 바뀌면 짝수/홀수가 유지됨)

또한 오류를 검출해도 어디서 발생했는지 알 수 없으므로 **정정할 수 없습니다.**

<br>

### CRC: 실용적인 오류 검출

현대 통신에서 가장 널리 사용되는 오류 검출 방식은 **CRC(Cyclic Redundancy Check)**입니다.

이더넷, USB, ZIP 파일 등에서 사용됩니다.

<br>

CRC는 다항식 나눗셈을 이용합니다.

데이터를 특정 다항식(생성 다항식)으로 나눈 나머지를 데이터에 붙여 전송합니다.

```
전송할 데이터: D
생성 다항식: G
나머지: R = D mod G
전송: D + R
```

수신측에서 (D + R)을 G로 나누면 나머지가 0이 되어야 합니다.

나머지가 0이 아니면 전송 중 오류가 발생한 것입니다.

<br>

CRC-32(32비트 체크섬)는 모든 1 ~ 32비트 버스트 오류를 검출합니다.

이더넷 프레임의 마지막 4바이트가 CRC-32 체크섬입니다.

<br>

### 해밍 코드: 오류를 정정하다

오류를 검출만 하면 재전송을 요청해야 합니다.

위성 통신처럼 왕복 시간이 긴 경우, 재전송 없이 **오류를 정정**할 수 있으면 좋겠습니다.

<br>

1950년, 리처드 해밍은 오류 정정 코드를 발명했습니다.

핵심 아이디어는 여러 개의 패리티 비트를 사용하여 오류 위치를 특정하는 것입니다.

<br>

해밍(7,4) 코드는 4비트 데이터를 7비트로 인코딩합니다.

3비트의 패리티를 추가합니다.

```
위치:  1   2   3   4   5   6   7
비트: P1  P2  D1  P3  D2  D3  D4

P1은 위치 1, 3, 5, 7을 검사 (이진수로 끝이 1인 위치)
P2는 위치 2, 3, 6, 7을 검사 (이진수로 둘째 자리가 1인 위치)
P3는 위치 4, 5, 6, 7을 검사 (이진수로 셋째 자리가 1인 위치)
```

오류가 발생하면 어떤 패리티가 맞지 않는지 확인합니다.

맞지 않는 패리티 비트 위치를 더하면 오류가 발생한 위치가 나옵니다.

```
P1, P3가 오류 → 오류 위치 = 1 + 4 = 5
해당 비트를 뒤집어 정정
```

<br>

해밍 코드의 원리는 현대의 더 강력한 오류 정정 코드들의 기초가 됩니다.

**LDPC(Low-Density Parity-Check)** 코드는 Wi-Fi 6, 5G, SSD에서 사용됩니다.

섀넌 한계에 매우 가깝게 접근하면서 실용적인 복잡도를 유지합니다.

<br>

### 재전송: ARQ

오류 정정 코드는 대역폭 오버헤드가 있습니다.

해밍(7,4)는 4비트 전송에 7비트를 사용합니다. 57% 효율입니다.

<br>

채널 상태가 좋을 때는 재전송이 더 효율적일 수 있습니다.

**ARQ(Automatic Repeat Request)**는 오류를 검출하면 재전송을 요청합니다.

<br>

가장 간단한 **Stop-and-Wait ARQ**:

```
송신: 프레임 1 전송
       ↓
수신: 검사 → 오류 없음 → ACK 전송
       ↓
송신: ACK 수신 → 프레임 2 전송
       ↓
수신: 검사 → 오류 발생 → NAK 전송
       ↓
송신: NAK 수신 → 프레임 2 재전송
```

<br>

Stop-and-Wait는 ACK를 기다리는 동안 채널이 놀게 됩니다.

**Go-Back-N**과 **Selective Repeat**은 여러 프레임을 연속 전송하여 효율을 높입니다.

TCP는 Selective Repeat과 유사한 방식을 사용합니다.

<br>

오류 정정과 재전송 중 어떤 것이 좋을까요?

채널 상태에 따라 다릅니다.

오류율이 낮으면 재전송이 효율적입니다.

오류율이 높거나 왕복 시간이 길면 오류 정정이 낫습니다.

현대 시스템은 둘을 조합합니다.

FEC로 일부 오류를 정정하고, 심각한 오류는 재전송으로 처리합니다.

---

## 채널을 나눠 쓰기

하나의 통신 채널을 여러 사용자가 공유해야 합니다.

무선 주파수 대역은 유한하고, 모든 사용자에게 별도 주파수를 할당할 수 없습니다.

어떻게 하나의 채널을 여러 사용자가 나눠 쓸 수 있을까요?

<br>

### FDMA: 주파수를 나누다

가장 직관적인 방법은 **FDMA(Frequency Division Multiple Access)**입니다.

전체 주파수 대역을 여러 개의 좁은 채널로 나누고, 각 사용자에게 하나씩 할당합니다.

```
주파수
  ↑
  │  ┌────────┐
  │  │ 사용자C │
  │  ├────────┤
  │  │ 사용자B │
  │  ├────────┤
  │  │ 사용자A │
  └──┴────────┴──→ 시간
```

FM 라디오가 이 방식입니다.

각 방송국이 다른 주파수를 사용합니다.

1세대(1G) 아날로그 이동통신도 FDMA를 사용했습니다.

<br>

FDMA의 문제는 낭비입니다.

사용자가 통화하지 않을 때도 주파수를 점유합니다.

또한 채널 간 간섭을 막기 위한 가드 밴드(사용하지 않는 주파수 간격)가 필요합니다.

<br>

### TDMA: 시간을 나누다

**TDMA(Time Division Multiple Access)**는 시간을 나눕니다.

모든 사용자가 같은 주파수를 사용하지만, 서로 다른 시간에 전송합니다.

```
     사용자A   사용자B   사용자C   사용자A   ...
    ┌────────┬────────┬────────┬────────┐
────┤  슬롯1 │  슬롯2 │  슬롯3 │  슬롯1 │───→ 시간
    └────────┴────────┴────────┴────────┘
```

각 사용자는 자기 타임 슬롯에서만 전송합니다.

GSM(2G)이 이 방식을 사용합니다.

<br>

TDMA는 모든 사용자가 정확히 동기화되어야 합니다.

약간의 타이밍 오차도 간섭을 일으킵니다.

<br>

### CDMA: 코드로 구분하다

**CDMA(Code Division Multiple Access)**는 발상의 전환입니다.

모든 사용자가 같은 주파수를, 같은 시간에 사용합니다.

대신 각 사용자에게 고유한 **코드**를 부여합니다.

<br>

각 사용자는 자신의 데이터에 고유 코드를 곱해서 전송합니다.

수신측은 원하는 사용자의 코드를 곱하면 해당 사용자의 데이터만 추출됩니다.

다른 사용자의 신호는 잡음처럼 분산됩니다.

<br>

이것이 가능한 이유는 **직교(Orthogonal)** 코드 때문입니다.

서로 다른 사용자의 코드를 곱하면 합이 0이 됩니다.

같은 코드끼리 곱하면 원래 데이터가 복원됩니다.

<br>

CDMA는 3G 이동통신(CDMA2000, WCDMA)에서 사용됩니다.

GPS 위성도 CDMA 방식으로 신호를 구분합니다.

<br>

### OFDMA: 시간과 주파수를 모두 활용

**OFDMA(Orthogonal Frequency Division Multiple Access)**는 현대 통신의 핵심입니다.

4G LTE, 5G, Wi-Fi 6가 이 방식을 사용합니다.

<br>

OFDMA는 먼저 넓은 주파수 대역을 수백~수천 개의 좁은 **부반송파(Subcarrier)**로 나눕니다.

이 부반송파들은 서로 직교하여 간섭 없이 겹칠 수 있습니다.

```
주파수
  ↑
  │   ∿   ∿   ∿   ∿   ∿   ∿   ∿   ∿
  │  ∿ ∿ ∿ ∿ ∿ ∿ ∿ ∿ ∿ ∿ ∿ ∿ ∿ ∿ ∿ ∿
  │ ∿   ∿   ∿   ∿   ∿   ∿   ∿   ∿   ∿
  └──────────────────────────────────→ 부반송파
```

<br>

그리고 이 부반송파들을 시간 슬롯별로 다른 사용자에게 할당합니다.

사용자 A는 특정 시간에 특정 부반송파들을 사용합니다.

```
주파수
  ↑
  │ ┌──┐┌──┐┌──┐┌──┐
  │ │A ││B ││A ││C │  ← 시간 슬롯 1
  │ ├──┤├──┤├──┤├──┤
  │ │B ││A ││C ││A │  ← 시간 슬롯 2
  └─┴──┴┴──┴┴──┴┴──┴──→ 부반송파
```

<br>

OFDMA의 강점은 유연성입니다.

채널 상태가 좋은 부반송파를 해당 사용자에게 할당할 수 있습니다.

많은 데이터가 필요한 사용자에게 더 많은 자원을 할당할 수 있습니다.

---

## 마무리: 정보의 물리적 한계

디지털 통신이 어떻게 물리적 채널의 한계 안에서 정보를 신뢰성 있게 전달하는지 살펴보았습니다.

<br>

디지털 방식의 핵심은 연속값 대신 이산 심볼을 사용하여 잡음 환경에서도 원본을 복원할 수 있다는 것입니다.

아날로그-디지털 변환은 나이퀴스트 정리가 정하는 샘플링 한계와 양자화 비트 수에 의해 결정됩니다.

섀넌의 정보 이론은 채널 용량이라는 절대적 한계를 제시하고, 그 한계 내에서 오류 없는 전송이 가능함을 보였습니다.

오류 검출과 정정은 중복을 추가하여 신뢰성을 확보합니다.

다중 접근 기술은 유한한 자원을 효율적으로 나눠 씁니다.

<br>

이 모든 것이 물리 계층과 데이터링크 계층에서 일어납니다.

[Part 3](/dev/network/NetworkCommunication-3/)에서는 이렇게 전달된 비트들이 어떻게 체계적으로 조직되어 의미 있는 통신이 되는지, 프로토콜 스택을 살펴봅니다.

<br>

---

**관련 글**
- [네트워크 통신의 원리 (1) - 전자기파와 신호 전송](/dev/network/NetworkCommunication-1/)
- [네트워크 통신의 원리 (3) - 프로토콜 스택과 데이터 흐름](/dev/network/NetworkCommunication-3/)
- [IP 주소(IP Address)의 개념과 구조](/dev/network/IPAddress/)
- [서브넷(Subnet)의 개념과 서브네팅](/dev/network/Subnet/)

